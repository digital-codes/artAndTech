{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41fa55c",
   "metadata": {},
   "source": [
    "Basic NLTK Example from https://www.dataknowsall.com/bowtfidf.html\n",
    "\n",
    "with some additions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# random features\n",
    "from random import sample\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import LegalitySyllableTokenizer\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2144097",
   "metadata": {},
   "source": [
    "A corpus is casically a list of sentences. YOu can create your own or use an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51807d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Tune a hyperparameter.\",\n",
    "    \"You can tune a piano but you cannot tune a fish.\",\n",
    "    \"Fish who eat fish, catch fish.\",\n",
    "    \"People can tune a fish or a hyperparameter.\",\n",
    "    \"It is hard to catch fish and tune it.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3e8fa",
   "metadata": {},
   "source": [
    "We do some standard text processing first\n",
    "First thing is to get the occurancies of all words in all lines. We use the count vectorizer to do this. Stop words are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with CountVectorizer which creates a BoW\n",
    "vectorizer = CountVectorizer(stop_words='english') \n",
    "X = vectorizer.fit_transform(corpus) \n",
    "pd.DataFrame(X.A, columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979fc596",
   "metadata": {},
   "source": [
    "The version above shows the actual occurancies. Now we use another vectorizer, which uses a different metric. We can configure this TfIdf (Term frequency-inverse document frequency, see e.g. [here](https://towardsdatascience.com/tf-idf-simplified-aba19d5f5530) ) vectorizer in two modes (\"use_idf\" fasle or true). False doens not consider the document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=False) \n",
    "X = vectorizer.fit_transform(corpus) \n",
    "df = pd.DataFrame(np.round(X.A,3), columns=vectorizer.get_feature_names_out())\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c886ea",
   "metadata": {},
   "source": [
    "use_idf = True uses the iverse document frequency, which favors words which are used in fewer lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf3edf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inverse vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True) \n",
    "X = vectorizer.fit_transform(corpus) \n",
    "df = pd.DataFrame(np.round(X.A,3), columns=vectorizer.get_feature_names_out())\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381214b4",
   "metadata": {},
   "source": [
    "Instead of counting words (somehow) we can also split lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc369b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize into words\n",
    "w = word_tokenize(\" \".join(corpus))\n",
    "print(\"Words: \",w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d9d81",
   "metadata": {},
   "source": [
    "We can also tokenize words into sylables\n",
    "\n",
    "\n",
    "**Note:** the tokenizer does not always produce the correct number of sylables ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9493162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sylable, single word\n",
    "SSP = SyllableTokenizer()\n",
    "s = SSP.tokenize('justification')\n",
    "print(\"Syllables: \",s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0187ae7",
   "metadata": {},
   "source": [
    "### After these basic examples we try to create lyrics\n",
    "A HAIKU A haiku is defined to have 3 lines with any number of words, provided the number of sylables is 5, 7 and 5 in the 3 lines.\n",
    "\n",
    "We start with the same corpus, but you may use some other text line or an NLTK sample corpus (check NLTK website)\n",
    "First thing to do here is to remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into single string and replace all punction with \" \"\n",
    "text = \" \".join(corpus)\n",
    "for p in punctuation:\n",
    "    text = text.replace(p,\" \")\n",
    "    \n",
    "print(\"Text: \",text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad9b67",
   "metadata": {},
   "source": [
    "Now we split into words (similar to example above) and create a dict with the number of sylables for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55308726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize into words\n",
    "words = word_tokenize(text)\n",
    "print(\"Words: \",words)\n",
    "\n",
    "# create dict with words and number of sylables\n",
    "wdict = {}\n",
    "for w in words:\n",
    "    if not w in wdict:\n",
    "        wdict[w] = len(SSP.tokenize(w))\n",
    "\n",
    "print(\"Wdict: \",wdict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dde925",
   "metadata": {},
   "source": [
    "Lets take a random sample from our dict (this will generate different results on every run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "rwords = sample(list(wdict.keys()), n)\n",
    "print(rwords)\n",
    "for r in rwords:\n",
    "    print(f\"{r}: {wdict[r]} sylables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc6038",
   "metadata": {},
   "source": [
    "### Up to you to create lines with the appropriate number of sylables for the haiku\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
